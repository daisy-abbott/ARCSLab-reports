{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f70c1a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdaisyabbott\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dcad2021/wandb/run-20230725_080831-cyflpuw4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/daisyabbott/first-testing-refactored/runs/cyflpuw4' target=\"_blank\">leafy-smoke-1</a></strong> to <a href='https://wandb.ai/daisyabbott/first-testing-refactored' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/daisyabbott/first-testing-refactored' target=\"_blank\">https://wandb.ai/daisyabbott/first-testing-refactored</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/daisyabbott/first-testing-refactored/runs/cyflpuw4' target=\"_blank\">https://wandb.ai/daisyabbott/first-testing-refactored/runs/cyflpuw4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact larger-perfect-dataset:v0, 893.10MB. 1852 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1852 of 1852 files downloaded.  \n",
      "Done. 0:0:0.3\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# Initialize W&B run (if not already initialized)\n",
    "run = wandb.init(\n",
    "    project=\"first-testing-refactored\",\n",
    "    entity=\"daisyabbott\",\n",
    "    notes=\"A set of small/useless datasets for testing.\",\n",
    "    job_type=\"dataset-upload\"\n",
    ")\n",
    "# Load the dataset artifact\n",
    "artifact = run.use_artifact(\"arcslaboratory/Multirun-testing-1K+/larger-perfect-dataset:v0\")\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "# Update the dataset path\n",
    "dataset_path = artifact_dir + \"/data/largedata\"  # Path to the extracted images from the artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94217ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c366389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from fastai.vision.all import *\n",
    "from fastai.callback.progress import CSVLogger\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2c7c7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation percent and num of epochs\n",
    "VALID_PCT = 0.05\n",
    "NUM_EPOCHS = 3\n",
    "NUM_REPLICATES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "732d76c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU: 3\n"
     ]
    }
   ],
   "source": [
    "# correct server num (using 4th for now, eventually switch to 3rd (2)and share w chau)\n",
    "torch.cuda.set_device(3) \n",
    "print(\"Running on GPU: \" + str(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "775d682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusted from initial raycasting file for simpler access\n",
    "from pathlib import Path\n",
    "current_dir = Path.cwd()\n",
    "relative_path = \"artifacts/larger-perfect-dataset:v0/data\"\n",
    "path1 = current_dir / relative_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f0579ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#1854) [Path('/home/dcad2021/artifacts/larger-perfect-dataset:v0/data/000000_0.png'),Path('/home/dcad2021/artifacts/larger-perfect-dataset:v0/data/000015_0.png'),Path('/home/dcad2021/artifacts/larger-perfect-dataset:v0/data/000060_0.png'),Path('/home/dcad2021/artifacts/larger-perfect-dataset:v0/data/000226_+0p70.png'),Path('/home/dcad2021/artifacts/larger-perfect-dataset:v0/data/000392_0.png'),Path('/home/dcad2021/artifacts/larger-perfect-dataset:v0/data/000075_0.png'),Path('/home/dcad2021/artifacts/larger-perfect-dataset:v0/data/000241_0.png'),Path('/home/dcad2021/artifacts/larger-perfect-dataset:v0/data/000135_0.png'),Path('/home/dcad2021/artifacts/larger-perfect-dataset:v0/data/000196_-0p88.png'),Path('/home/dcad2021/artifacts/larger-perfect-dataset:v0/data/000346_0.png')...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path1.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2ce5e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_image_files(path1)\n",
    "use_pretraining = True\n",
    "rgb_instead_of_gray = True \n",
    "rep = 1\n",
    "model_name = \"resnet18\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e38b45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_prefix = \"classification-\" + model_name\n",
    "file_prefix += '-rgb' if rgb_instead_of_gray else '-gray'\n",
    "file_prefix += '-pretrained' if use_pretraining else '-notpretrained'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa043da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "compared_models = {\n",
    "    \"resnet18\": resnet18,\n",
    "    # \"resnet34\": resnet34\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e75d155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model relative filename : /home/dcad2021/artifacts/larger-perfect-dataset:v0/data/classification-resnet18-rgb-pretrained-1.pkl\n",
      "Log relative filename   : /home/dcad2021/artifacts/larger-perfect-dataset:v0/data/classification-resnet18-rgb-pretrained-trainlog-1.csv\n",
      "Log relative filename   : /home/dcad2021/artifacts/larger-perfect-dataset:v0/data/classification-resnet18-rgb-pretrained-trainlog-1.csv\n"
     ]
    }
   ],
   "source": [
    "# I may need to double check the vars in this\n",
    "model_filename = path1 / f\"{file_prefix}-{rep}.pkl\"\n",
    "print(\"Model relative filename :\", model_filename)\n",
    "log_filename = path1 / f\"{file_prefix}-trainlog-{rep}.csv\"\n",
    "print(\"Log relative filename   :\", log_filename)\n",
    "print(\"Log relative filename   :\", log_filename)\n",
    "fig_filename_prefix = path1 / file_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b977b4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fig_filename(prefix: str, label: str, ext: str, rep: int) -> str:\n",
    "    fig_filename = f\"{prefix}-{label}-{rep}.{ext}\"\n",
    "    print(label, \"filename :\", fig_filename)\n",
    "    return fig_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "114f4e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filename_to_class(filename: str) -> str:\n",
    "    angle = float(filename.split(\"_\")[1].split(\".\")[0].replace(\"p\", \".\"))\n",
    "    if angle > 0:\n",
    "        return \"left\"\n",
    "    elif angle < 0:\n",
    "        return \"right\"\n",
    "    else:\n",
    "        return \"forward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d7b5737",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageWithCmdDataset(Dataset):\n",
    "    def __init__(self, filenames):\n",
    "        \"\"\"\n",
    "        Creates objects for class labels, class indices, and filenames.\n",
    "        \n",
    "        :param filenames: (list) a list of filenames that make up the dataset\n",
    "        \"\"\"\n",
    "        self.class_labels = ['left', 'forward', 'right']\n",
    "        self.class_indices = {lbl:i for i, lbl in enumerate(self.class_labels)} # {'left': 0, 'forward': 1, 'right': 2}        \n",
    "        self.all_filenames = filenames\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Gives length of dataset.\n",
    "        \n",
    "        :return: (int) the number of filenames in the dataset\n",
    "        \"\"\"\n",
    "        return len(self.all_filenames)\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Gets the filename associated with the given index, opens the image at\n",
    "        that index, then uses the image's filename to get information associated\n",
    "        with the image such as its label and the label of the previous image.\n",
    "        \n",
    "        :param index: (int) number that represents the location of the desired data\n",
    "        :return: (tuple) tuple of all the information associated with the desired data\n",
    "        \"\"\"\n",
    "        # The filename of the image given a specific index\n",
    "        img_filename = self.all_filenames[index]            \n",
    "        \n",
    "        # Opens image file and ensures dimension of channels included\n",
    "        img = Image.open(img_filename).convert('RGB')\n",
    "        # Resizes the image\n",
    "        img = img.resize((224, 224))\n",
    "        # Converts the image to tensor and \n",
    "        img = torch.Tensor(np.array(img)/255)\n",
    "        # changes the order of the dimensions\n",
    "        img = img.permute(2,0,1)\n",
    "        \n",
    "        # Getting the current image's label\n",
    "        label_name = filename_to_class(img_filename)\n",
    "        label = self.class_indices[label_name]\n",
    "        \n",
    "        # Getting the previous image's label\n",
    "        # The default is 'forward'\n",
    "        cmd_name = 'forward'\n",
    "        \n",
    "        # If the index is not 0, the cmd is determined by the previous img_filename\n",
    "        if index != 0:\n",
    "            prev_img_filename = self.all_filenames[index-1]\n",
    "            cmd_name = filename_to_class(prev_img_filename)            \n",
    "        cmd = self.class_indices[cmd_name]\n",
    "        \n",
    "        # Data and the label associated with that data\n",
    "        return (img, cmd), label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "615da112",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = ImageDataLoaders.from_name_func(path1, files, filename_to_class, valid_pct = VALID_PCT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "518b2bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.savefig(get_fig_filename(\"batch\", \"pdf\", rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96e04b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cmd_model(nn.Module):\n",
    "    def __init__(self, arch: str, pretrained: bool):\n",
    "        super(cmd_model, self).__init__()\n",
    "        self.cnn = arch(pretrained=pretrained)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.cnn.fc.out_features + 1, 512)\n",
    "        self.r1 = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Linear(512, 3)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        print(data)\n",
    "        img, cmd = data\n",
    "        x1 = self.cnn(img)\n",
    "        x2 = cmd.unsqueeze(1)\n",
    "        \n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.r1(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77e8566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloaders(dataset_name: str, prefix: str) -> DataLoaders:\n",
    "\n",
    "    path = path1\n",
    "    files = get_image_files(path1)\n",
    "    \n",
    "    # Get size of dataset and corresponding list of indices\n",
    "    dataset_size = len(files)\n",
    "    dataset_indices = list(range(dataset_size))\n",
    "    \n",
    "    # Shuffle the indices\n",
    "    np.random.shuffle(dataset_indices)\n",
    "\n",
    "    # Get the index for where we want to split the data\n",
    "    val_split_index = int(np.floor(VALID_PCT * dataset_size))\n",
    "    \n",
    "    # Split the list of indices into training and validation indices\n",
    "    train_idx, val_idx = dataset_indices[val_split_index:], dataset_indices[:val_split_index]\n",
    "    \n",
    "    # Get the list of filenames for the training and validation sets\n",
    "    train_filenames = [files[i] for i in train_idx]\n",
    "    val_filenames = [files[i] for i in val_idx]\n",
    "    \n",
    "    # Create training and validation datasets\n",
    "    train_data = ImageWithCmdDataset(train_filenames)\n",
    "#     train_data.__get_item__(10)\n",
    "    val_data = ImageWithCmdDataset(val_filenames)\n",
    "    \n",
    "    # Get DataLoader\n",
    "    dls = DataLoaders.from_dsets(train_data, val_data)\n",
    "    dls = dls.cuda()\n",
    "\n",
    "    #dls.show_batch()  # type: ignore\n",
    "    plt.savefig(get_fig_filename(prefix, \"batch\", \"pdf\", 0))\n",
    "\n",
    "    return dls  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d2ba64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    dls: DataLoaders,\n",
    "    model_arch: str,\n",
    "    pretrained: bool,\n",
    "    logname: Path,\n",
    "    modelname: Path,\n",
    "    prefix: str,\n",
    "    rep: int,\n",
    "):\n",
    "    arch = compared_models[model_arch]\n",
    "    net = cmd_model(arch, pretrained=pretrained)\n",
    "    \n",
    "    learn = Learner(\n",
    "        dls,\n",
    "        net,\n",
    "        loss_func=CrossEntropyLossFlat(),\n",
    "        metrics=accuracy,\n",
    "        cbs=CSVLogger(fname=logname),\n",
    "    )\n",
    "\n",
    "    if pretrained:\n",
    "        learn.fine_tune(NUM_EPOCHS)\n",
    "    else:\n",
    "        learn.fit_one_cycle(NUM_EPOCHS)\n",
    "\n",
    "    # Save trained model\n",
    "    torch.save(net.state_dict(), modelname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de2a8dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mambaforge/envs/fastai/lib/python3.10/site-packages/fastai/vision/learner.py:288: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n",
      "  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n",
      "/opt/mambaforge/envs/fastai/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/mambaforge/envs/fastai/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "learn = cnn_learner(dls, compared_models[model_name], metrics=accuracy, pretrained=use_pretraining, cbs=CSVLogger(fname=log_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "118747fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/home/dcad2021/artifacts/larger-perfect-dataset:v0/data')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acdbb929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.393035</td>\n",
       "      <td>0.564179</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>00:34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.584510</td>\n",
       "      <td>0.509490</td>\n",
       "      <td>0.858696</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.346169</td>\n",
       "      <td>0.186851</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.212091</td>\n",
       "      <td>0.136459</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if use_pretraining:\n",
    "    learn.fine_tune(NUM_EPOCHS)\n",
    "else:\n",
    "    learn.fit_one_cycle(NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20389174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    arg_parser = ArgumentParser(\"Train cmd classification networks.\")\n",
    "    arg_parser.add_argument(\n",
    "        \"model_arch\", help=\"Model architecture (see code for options)\"\n",
    "    )\n",
    "    arg_parser.add_argument(\n",
    "        \"dataset_name\", help=\"Name of dataset to use (corrected-wander-full)\"\n",
    "    )\n",
    "    arg_parser.add_argument(\n",
    "        \"--pretrained\", action=\"store_true\", help=\"Use pretrained model\"\n",
    "    )\n",
    "\n",
    "    args = arg_parser.parse_args()\n",
    "    dls = prepare_dataloaders(dataset_path, fig_filename_prefix)\n",
    "    \n",
    "    # Train NUM_REPLICATES separate instances of this model and dataset\n",
    "    for rep in range(NUM_REPLICATES):\n",
    "         train_model(\n",
    "            dls,\n",
    "            args.model_arch,\n",
    "            args.pretrained,\n",
    "            log_filename,\n",
    "            model_filename,\n",
    "            fig_filename_prefix,\n",
    "            rep,\n",
    "        )\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360f4d04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

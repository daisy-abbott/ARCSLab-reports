# Summary 
| Date   | Notes
| :----- | :-------------------------------
| 7/3    | Finished SWOT, installed and experimented with blender
| 7/4    | off
| 7/5    | Read through research paper again, created SweetHome Key, started transferring to Blender. 
| 7/6    | Article Meeting, Swot meeting, cloned gitea, tested new model, could not find screenshots. 
| 7/7    | Continued building oldenborg with blender. 

# Activites 
* Installed blender
    * Experimented with blender: 
    * <img width="1440" alt="blenderExperiment" src="https://github.com/daisy-abbott/ARCSLab-reports/assets/112681549/91bd12d5-7984-4ab2-b6c4-368ba8833cfb">

* Used Odlenborg Floor Plan to create a Model Key for Blender. 
    * Scaled it with the southwest hallway: 36'
    * <img width="1440" alt="oldyScale" src="https://github.com/daisy-abbott/ARCSLab-reports/assets/112681549/52632a7a-88a6-4305-89a8-d63daf75e530">
    * mapped out width and length (not height) of oldy. Most walls are 8'. Using height of stairs we can estimate slopes but might be good to get the actual degree with a level (?)
    * <img width="1440" alt="2Dimage" src="https://github.com/daisy-abbott/ARCSLab-reports/assets/112681549/bf7f1b9c-0dd1-4307-9c6e-4e22987f5f69">
    * Model: LINK (push to git then paste its link)

* Used Model Key to create Oldenborg in Blender 
* Progress so far: 
<img width="1412" alt="overview7:8" src="https://github.com/daisy-abbott/ARCSLab-reports/assets/112681549/b058a3f9-6fd3-4755-9ca9-7eca1516dabf">
* hallway:
<img width="1372" alt="rampblender" src="https://github.com/daisy-abbott/ARCSLab-reports/assets/112681549/29477958-59e6-4124-a231-8bcec74d02e1">

* stairs:
<img width="1429" alt="stairs" src="https://github.com/daisy-abbott/ARCSLab-reports/assets/112681549/0d06ca22-ae36-479b-a064-6013b8455151">

* Cloned ArcsAssets from gitea and Tested new packaged model with commands
* <img width="831" alt="printsc" src="https://github.com/daisy-abbott/ARCSLab-reports/assets/112681549/ccf48b57-75ff-4051-9e4e-04f379439d06">

# Blender Overview
* It's definitely time consuming to line everything up, and to calculate the angles and height and rotations, but I think it could work! I'm not adding in any doors or windows yet... would that be an easy addition in unreal? If not I will start on that after the skeleton is completed. 

# Issues
* Can't find screenshots from OSC packaged model unreal
* one stair seems to be floating from the side even though other angles it looks fine and dimensions check out. 
* <img width="1414" alt="messedupstairs" src="https://github.com/daisy-abbott/ARCSLab-reports/assets/112681549/930296ca-04bf-47a4-99b7-9a62a07a16d2">
* the stairs don't seem to be the right dimensions
* <img width="1358" alt="stairdimwrong" src="https://github.com/daisy-abbott/ARCSLab-reports/assets/112681549/74e7e9ad-c3c2-4ce4-8a4d-1b8752bf9b35">
* Some of the hallway entrances look way too narrow. This is from the floor plan, even though the dimensions match. 
* <img width="1413" alt="narrowprob" src="https://github.com/daisy-abbott/ARCSLab-reports/assets/112681549/dc288104-bf0f-452b-8b00-be12498875bc">
* Floor plan: 
<img width="917" alt="SHnarrow" src="https://github.com/daisy-abbott/ARCSLab-reports/assets/112681549/d9628657-644c-4acd-9fa1-4a5221bed2b6">


# Plans
* fix blender model issues


# Articles
* Gibson: 
* Roll, pitch, and yaw angles are the angles of rotation about each axis respectively. 
* "A point cloud is essentially a collection of individual points, where each point contains information about its location or spatial coordinates in three-dimensional space. Each point in the point cloud represents the position of a specific feature, object, or surface within the environment being captured or modeled." 
* Geometric point cloud rendering, they transform the given panoramas into point clouds. Each pixel is projected from equirectangular (spherical images on rectangle images) to Cartesian (x,y,z with rotational aspects, roll, pitch, yaw). Then for the desired target view, they choose the next closest images in the dataset, and project back onto equirectangular view. This can cause some gaps and pixel that are supposed to be hidden can show through. To fix this, they render a depth and filter them by similar depth so that the ones with > 0.1 depth will be hidden. 
* Bilinear interpolation is a tecnique used to estimate a point between two neighboring points. 
* Stochastic identity initialization is a technique used in machine learning to initialize the parameters of a neural network or model with random values close to an identity matrix. It introduces randomness in the initialization process, which helps prevent symmetries and encourage diverse learning patterns in the network
* A proprioceptive sensor suite is a collection of sensors that provide information about the internal state or motion of an organism or a robotic system. These sensors measure the body's own position, orientation, joint angles, and other internal parameters. They enable the system to perceive and understand its own body's movements, allowing for self-awareness and control.
* Benchmark: Gibson benchmarked its space database by comparing it to existing synthetic and real databases, namely SUNCG and Matterport3D.
* Effectiveness of Goggles;  The gold standard is train and test on
It (real images) which yields the error of 0.86. The closest combination to that in the entire table is train on f(Is)
(f output) and test on u(It) (real image through Goggles)
giving 0.91, which signifies the effectiveness of Goggles
* Paired data refers to a type of data where each observation in one dataset is associated or matched with a corresponding observation in another dataset.

# Questions: 
1. The paper talks about replicating the real world both visually and semantically. The first I get, scanned real spaces, but the latter, semantic complexity, I'm not entirely clear on. Is that like different things that could happen, interference, moving obejects, or more like replicating ongoing actions? Or just interactions between objects? 
2. At the top of page 2, they mention classic control, can you explain what that means? 
2. Can you explain domain randomization? 
3. Can we go over geometric point cloud rendering? 
4. Can we go over the section for loss? 
5. What are conventional computer vision datasets? 
6. Can you explain the formula where distance is multiplied by the hyperparameter lambda d? 
7. Can you explain why, (at the top of page 5)the expression needs to be altered? Like why would all of the images be at risk of collapsing to a single point? 
8. It was interesting to see the benchmark, and that Gibson had a higher value of Specific Surface Area, indicating more complex and detailed surfaces. But for Navigation Complexity, Gibson was in the middle, and lower is better right? So Gibson didn't perform as well due to navigational distances or something with A*? Real world transfer, Gibson had the BEST depth estimation and accuracy. Gibson also had the greatest variety of scenes present in the database. 
    * In terms of Benchmarking, Gibson outperformed SUNCG and Matterport in every area except for Navigation. Why was that? 
9. why does paired data improve results so much? 

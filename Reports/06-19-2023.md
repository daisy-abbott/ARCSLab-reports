# Summary 
| Date   | Notes
| :----- | :-------------------------------
| 6/20   | Group Meeting, completed W&B tutorial, started MlOps with W&B, started another oldenborg experiementing with stairs and incline 
| 6/21   | Meeting with Clark on kernal and git issues, continued with MLOps
| 6/22   | Made staircase in Sweet Home, continued building new model in SH, continued MLOps course
| 6/23   | Started reading the research paper that Liz put in slack, continued MLOps course, installed unreal, installed xcode, tried to import


# Activities 
* W&B Course Notes
    * lesson one: Building an end to end prototype
        * goal: recognize what's in front of car (self driving)
        * exploratory data analysis: weights and bias tables
        * Split data into train, validation and test, then trained the baseline model
        * communicate results in reports
        * Started following along: https://github.com/daisy-abbott/ARCSLab-reports/blob/master/Assets/MLOPSnotebook1.ipynb

    * Lesson two: Moving beyond the baseline
        * Can refactor baseline to improve results (re-organize code)
        * can move notebook to a .py file and then run through terminal
            * can see runs in workspace
        * --help shows all the parameters you are capable of overriding 
        * can adjust batch size: --batch_size 16
        * IOU = intersection over union
        * Add sweep configuation file to adjust hyper paremeters
        * Can run on multiple GPU's and reduce time performing the sweep 
        * Summary: Optimizing hyperparameters, experimentation on a large scale. Code organization, conductive experiments, analyzing experiments.WB experiments, reports and sweepts. Baseline: use jupyter notebook. Then refactor into python script with argpars. Then used sweeps to give visuals and found best hyperparemeters, additional visuals correlation plots, Random vs. Bayesian. After sweep, had hundreds of runs: Can use grouping, sorting, or filter runs. Can use reports to summarize findings. 

    * Lesson Three: Model Evaluation: 
        * Data Partitioning: Training, validation, holdout. Most randomly split data? But they: avoided data leakage by not allowing related images to belong to separate sets. Concept is called group partitioning. 
        * Stratified Partitioning: Instead of random, fairly represent all of the classes in your data. 
        * Time series: Observing something over time, can't randomly split data, will leak data from the future. 
        * IMG Try to predict N steps ahead. 
        * Other types of Data Partitioning: K-Fold Cross validation is useful when you have a small amount of data and want multiple estimates of performance. Random Train/Validation/Holdout Split. Good for when there is no time element and no information leagae across examples. 
        * Choosing an evaluation metric: Pick a single number: IE precision and recall are two. Consider just F1 score. Or another single metric or minimum threshold. Try to only optimize one metric, ie Accuracy. 

* Downloaded Unreal and Opened zipped package
    * error and needed to install XCode to launch
    * 


* Sweet Home Staircases: 
    * Cannot easily add in staircases or inclines to existing models. 
    * <img width="1000" alt="stairoldynotwork" src="https://github.com/daisy-abbott/ARCSLab-reports/assets/112681549/70541a50-a938-4a17-ab48-a092c259d413"> 
    * Must Add another elevation. Plan -> add level. Then import original background image with the same origin and dimensions. Then can right click on level -> modify level -> adjust elevation to bring it down. 
    * <img width="1000" alt="stairsoldy" src="https://github.com/daisy-abbott/ARCSLab-reports/assets/112681549/16e299b4-ec55-4fe0-b4b8-27e97bbf447a"> 

    * This possibly isn't a great place to put the stairs, not a lot of room with what we are working with. 

    * <img width="928" alt="2ndroomoldy" src="https://github.com/daisy-abbott/ARCSLab-reports/assets/112681549/f4c4c1f5-b893-469a-8546-18cea163333c">

    * <img width="935" alt="oldyroom" src="https://github.com/daisy-abbott/ARCSLab-reports/assets/112681549/a511805c-d520-4db6-9511-aae608b5f808">

* <img width="936" alt="2ndstairs" src="https://github.com/daisy-abbott/ARCSLab-reports/assets/112681549/f76f6f61-6760-46dc-be14-87c5d3f7271e">

# Issues
* So far unable to import packaged model, possible that it is waiting to upload. 
# Plans 
Create the dataset from packaged model 

# Articles
* Started reading Gibson Env: Real-World Perception for Embodied Agents: https://arxiv.org/pdf/1808.10654.pdf
    * visuals are from real world rather than artificially designed space
    * Summary of Abstract: Since developing visual perception models and sensorimotor control in the real world is difficult, learning in simulations has become popular because it's faster and less costly (no robots are damaged during training). This posses another challenge of bridging the reality gap, can the agent perform in reality as well as it did in the simulation? The Gibson Virtual Environment is a virtual representation of real spaces. Gibson 1) is from the real world, 2) has internal synthesis mechanism, 3) forces agents to be subject to the constraints of physics and space. 
    * Intro: 
        * 

# Summary 
| Date   | Notes
| :----- | :-------------------------------
| 6/26   | Finished Real-World Perception for Embodied Agents, Continued MLOps course, got packaged model running
| 6/27   | 
| 6/28   | 
| 6/29   |  

# Activities
* Finished Gibson Env: Real-World Perception for Embodied Agents: https://arxiv.org/pdf/1808.10654.pdf (more under articles)
* Got packaged model of Unreal running
* Continued MLOps Course: 
    * 
# Issues
* unable to fin screenshots from packaged model
# Plans 

# Articles
Gibson Env: Real-World Perception for Embodied Agents: https://arxiv.org/pdf/1808.10654.pdf
* visuals are from real world rather than artificially designed space
* Summary of Abstract: Since developing visual perception models and sensorimotor control in the real world is difficult, learning in simulations has become popular because it's faster and less costly (no robots are damaged during training). This posses another challenge of bridging the reality gap, can the agent perform in reality as well as it did in the simulation? The Gibson Virtual Environment is a virtual representation of real spaces. Gibson 1) is from the real world, 2) has internal synthesis mechanism, 3) forces agents to be subject to the constraints of physics and space. 

* Intro: Gibson's main goal is the successful transferring from simulated model to real time real world images. This is done by scanning the real world and replicating its semantic complexity. Accomplished through a neural network based rendering approach which simulates forward and backward motion, corrective glasses for the agent "goggles".

* Related Work: Conventionally, vision is learned in static datasets, but pre recorded videos and photos are limited. Virtual environments are more effective for training and benchmarking learning algorithms. Many of these VE's are limited as they are oversimplified. Gibson addresses these limitations through: a custom neural view synthesizer, and a baked-in adaption mechanism: Goggles. When multiple images are available, can used ImageBasedRendering to help with lighting inconsistencies etc. 

* Real World Perceptual Environment: Datasets contain 3D reconstructed mesh panoramas. Their view synthesis module takes a sparse set of RGB-D
panoramas in the input and renders a panorama from an arbitrary novel viewpoint. A point cloud: a collection of 3D points in space that represent the shape and spatial distribution of objects or surfaces. Point clouds are generated and then projected onto images to synthesize desired views. (taken from 2d equirectangular to 3d cartisan coordinates and project back to 2d). They develop a stochastic approach to initializing the network at identity,to keep the weights nearly randomly distributed
* Closing the Gap: Form a joint space to dissolve the reality gap. Created a function U T -> S. The network U is trained to alter an image taken in real-world, It, to look like the corresponding rendered image in Gibson, Is, after passing through network f. Named, Corrective Glasses: Goggles. They integrated Gibson with a physics engine PyBullet, to uphold percedption and adhere to the laws of physics. 
* Tasks: Validate with obstacle avoidance, Distant Visual Navigation, Stair Climb.
* Experimental Results:  The effectiveness of Goggles is demonstrated through comparisons of different domain transfers in the context of static-recognition tasks, such as scene classification and depth estimation. The performance of these transfers is evaluated and compared, indicating how well Goggles reduces the domain gap between rendered images and real-world imagery.
* Conclusion: Verified effectiveness of Goggles but have not yet incorporated moving objects or any other dynamic contnet. In the future, plan to evaluate Goggles on real robots. 


# Questions: 
* Can you explain domain randomization? 
* Can we go over geometric point cloud rendering? 
   *  Point clouds are generated and then projected onto images to synthesize desired views. 
* Can we go over the section for Loss? 
